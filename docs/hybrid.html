<!DOCTYPE html>
<html><head><meta charset="utf-8"></meta><title>Annonated Algorithm Visualization</title><link rel="stylesheet" href="pylit.css?v=1"></link><link rel="stylesheet" href="solarized.css"></link><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css" integrity="sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/" crossorigin="anonymous"></link><script src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js" integrity="sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);" defer="True"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/lib/codemirror.min.css"></link><script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/lib/codemirror.min.js"></script><script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/mode/python/python.min.js"></script></head><body><div class="section" id="section-0"><div class="docs doc-strings"><p><p><a href="index.html"><b>HOME<br></b></a>PyTorch demo of PPO algorithm in hybrid action space.</p></p><a href="https://github.com/opendilab/PPOxFamily" target="_blank"><img alt="GitHub" style="max-width:100%;" src="https://img.shields.io/github/stars/opendilab/PPOxFamily?style=social"></img></a>  <a href="https://space.bilibili.com/1112854351?spm_id_from=333.337.0.0" target="_blank"><img alt="bilibili" style="max-width:100%;" src="https://img.shields.io/badge/bilibili-video%20course-blue"></img></a>  <a href="https://twitter.com/OpenDILab" rel="nofollow" target="_blank"><img alt="twitter" style="max-width:100%;" src="https://img.shields.io/twitter/follow/opendilab?style=social"></img></a><br><a href="https://github.com/opendilab/PPOxFamily/tree/main/chapter2_action/hybrid_tutorial.py" target="_blank">View code on GitHub</a></div></div><div class="section" id="section-1"><div class="docs doc-strings"><p>        <b>Overview</b><br>            The definition of hybrid action policy network used in PPO, which is mainly composed<br>            of three parts: encoder, action_type head (discrete) and action_args head (continuous).</p></div><div class="code"><pre><code id="code_1" name="py_code">from typing import Dict
import torch
import torch.nn as nn
import treetensor.torch as ttorch
from torch.distributions import Normal, Independent


class HybridPolicyNetwork(nn.Module):
    def __init__(self, obs_shape: int, action_shape: Dict[str, int]) -> None:</code></pre></div></div><div class="section" id="section-3"><div class="docs doc-strings"><p>        PyTorch necessary requirements for extending <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">nn.Module</span> .</p></div><div class="code"><pre><code id="code_3" name="py_code">        super(HybridPolicyNetwork, self).__init__()</code></pre></div></div><div class="section" id="section-4"><div class="docs doc-strings"><p>        Define encoder module, which maps raw state into embedding vector.<br>        It could be different for various state, such as Convolution Neural Network for image state.<br>        Here we use two-layer MLP for vector state.</p></div><div class="code"><pre><code id="code_4" name="py_code">        self.encoder = nn.Sequential(
            nn.Linear(obs_shape, 16),
            nn.ReLU(),
            nn.Linear(16, 32),
            nn.ReLU(),
        )</code></pre></div></div><div class="section" id="section-5"><div class="docs doc-strings"><p>        Define action_type head module, which outputs discrete logit.</p></div><div class="code"><pre><code id="code_5" name="py_code">        self.action_type_shape = action_shape['action_type_shape']
        self.action_type_head = nn.Linear(32, self.action_type_shape)</code></pre></div></div><div class="section" id="section-6"><div class="docs doc-strings"><p>        Define action_args head module, which outputs corresponding continuous action arguments.</p></div><div class="code"><pre><code id="code_6" name="py_code">        self.action_args_shape = action_shape['action_args_shape']
        self.action_args_mu = nn.Linear(32, self.action_args_shape)
        self.action_args_log_sigma = nn.Parameter(torch.zeros(1, self.action_args_shape))
</code></pre></div></div><div class="section" id="section-7"><div class="docs doc-strings"><p>        <b>Overview</b><br>            The computation graph of hybrid action policy network used in PPO.</p></div><div class="code"><pre><code id="code_7" name="py_code">    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:</code></pre></div></div><div class="section" id="section-9"><div class="docs doc-strings"><p>        Transform original state into embedding vector (i.e. (B, *) -> (B, N)).</p></div><div class="code"><pre><code id="code_9" name="py_code">        x = self.encoder(x)</code></pre></div></div><div class="section" id="section-10"><div class="docs doc-strings"><p>        Output discrete action logit.</p></div><div class="code"><pre><code id="code_10" name="py_code">        logit = self.action_type_head(x)</code></pre></div></div><div class="section" id="section-11"><div class="docs doc-strings"><p>        Output the argument mu depending on the embedding vector.</p></div><div class="code"><pre><code id="code_11" name="py_code">        mu = self.action_args_mu(x)</code></pre></div></div><div class="section" id="section-12"><div class="docs doc-strings"><p>        Utilize broadcast mechanism to make the same shape between log_sigma and mu.<br>        <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">zeros_like</span> operation doesn't pass gradient.<br>        <a href="https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html#in-brief-tensor-broadcasting">Related Link</a></p></div><div class="code"><pre><code id="code_12" name="py_code">        log_sigma = self.action_args_log_sigma + torch.zeros_like(mu)</code></pre></div></div><div class="section" id="section-13"><div class="docs doc-strings"><p>        Utilize exponential operation to produce the actual sigma.</p></div><div class="code"><pre><code id="code_13" name="py_code">        sigma = torch.exp(log_sigma)</code></pre></div></div><div class="section" id="section-14"><div class="docs doc-strings"><p>        Return treetensor-type output.</p></div><div class="code"><pre><code id="code_14" name="py_code">        return ttorch.as_tensor({
            'action_type': logit,
            'action_args': {
                'mu': mu,
                'sigma': sigma
            }
        })

</code></pre></div></div><div class="section" id="section-15"><div class="docs doc-strings"><p>    <b>Overview</b><br>        The function of sampling hybrid action, input is a dict with two keys 'mu' and 'sigma',<br>        both of them has shape = (B, action_shape), output shape = (B, action_shape).<br>        In this example, batch_shape = (B, ), event_shape = (action_shape, ), sample_shape = ().</p></div><div class="code"><pre><code id="code_15" name="py_code">def sample_hybrid_action(logit: ttorch.Tensor) -> torch.Tensor:</code></pre></div></div><div class="section" id="section-17"><div class="docs doc-strings"><p>    Transform logit (raw output of discrete policy head, e.g. last fully connected layer) into probability.</p></div><div class="code"><pre><code id="code_17" name="py_code">    prob = torch.softmax(logit.action_type, dim=-1)</code></pre></div></div><div class="section" id="section-18"><div class="docs doc-strings"><p>    Construct categorical distribution.<br>    <a href="https://en.wikipedia.org/wiki/Categorical_distribution">Related Link</a></p></div><div class="code"><pre><code id="code_18" name="py_code">    discrete_dist = torch.distributions.Categorical(probs=prob)</code></pre></div></div><div class="section" id="section-19"><div class="docs doc-strings"><p>    Sample one discrete action type per sample</p></div><div class="code"><pre><code id="code_19" name="py_code">    action_type = discrete_dist.sample()
</code></pre></div></div><div class="section" id="section-20"><div class="docs doc-strings"><p>    Construct gaussian distribution with $$\mu, \sigma$$<br>    <a href="https://en.wikipedia.org/wiki/Normal_distribution">Related Link</a></p></div><div class="code"><pre><code id="code_20" name="py_code">    continuous_dist = Normal(logit.action_args.mu, logit.action_args.sigma)</code></pre></div></div><div class="section" id="section-21"><div class="docs doc-strings"><p>    Reinterpret <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">action_shape</span> gaussian distribution into a multivariate gaussian distribution with<br>    diagonal convariance matrix.<br>    Ensure each event is independent with each other.<br>    <a href="https://pytorch.org/docs/stable/distributions.html#independent">Related Link</a></p></div><div class="code"><pre><code id="code_21" name="py_code">    continuous_dist = Independent(continuous_dist, 1)</code></pre></div></div><div class="section" id="section-22"><div class="docs doc-strings"><p>    Sample one action args of the shape <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">action_shape</span> per sample.</p></div><div class="code"><pre><code id="code_22" name="py_code">    action_args = continuous_dist.sample()</code></pre></div></div><div class="section" id="section-23"><div class="docs doc-strings"><p>    Return the final parameterized action.</p></div><div class="code"><pre><code id="code_23" name="py_code">    return ttorch.as_tensor({
        'action_type': action_type,
        'action_args': action_args,
    })

</code></pre></div></div><div class="section" id="section-24"><div class="docs doc-strings"><p>    <b>Overview</b><br>        The function of testing sampling hybrid action. Construct a hybrid action (parameterized action)<br>        policy and sample a group of action.</p></div><div class="code"><pre><code id="code_24" name="py_code">def test_sample_hybrid_action():</code></pre></div></div><div class="section" id="section-26"><div class="docs doc-strings"><p>    Set batch_size = 4, obs_shape = 10, action_shape is a dict, including 3 possible discrete<br>    action types and 3 corresponding continuous arguments. The relationship between action_type<br>    and action_args are represented by the below <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">mask</span> .</p></div><div class="code"><pre><code id="code_26" name="py_code">    B, obs_shape, action_shape = 4, 10, {'action_type_shape': 3, 'action_args_shape': 3}
    mask = [[0, 1, 0], [1, 0, 0], [0, 0, 1]]</code></pre></div></div><div class="section" id="section-27"><div class="docs doc-strings"><p>    Generate state data from uniform distribution.</p></div><div class="code"><pre><code id="code_27" name="py_code">    state = torch.rand(B, obs_shape)</code></pre></div></div><div class="section" id="section-28"><div class="docs doc-strings"><p>    Define hybrid action network with encoder, discrete head and continuous head.</p></div><div class="code"><pre><code id="code_28" name="py_code">    policy_network = HybridPolicyNetwork(obs_shape, action_shape)</code></pre></div></div><div class="section" id="section-29"><div class="docs doc-strings"><p>    Policy network forward procedure, input state and output dict-type logit.</p></div><div class="code"><pre><code id="code_29" name="py_code">    logit = policy_network(state)
    assert isinstance(logit, ttorch.Tensor)
    assert logit.action_type.shape == (B, action_shape['action_type_shape'])
    assert logit.action_args.mu.shape == (B, action_shape['action_args_shape'])
    assert logit.action_args.sigma.shape == (B, action_shape['action_args_shape'])</code></pre></div></div><div class="section" id="section-30"><div class="docs doc-strings"><p>    Sample action accoding to corresponding logit part.</p></div><div class="code"><pre><code id="code_30" name="py_code">    action = sample_hybrid_action(logit)
    assert action.action_type.shape == (B, )
    assert action.action_args.shape == (B, action_shape['action_args_shape'])</code></pre></div></div><div class="section" id="section-31"><div class="docs doc-strings"><p>    Acquire each sample's mask by looking up in <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">mask</span> with action type。</p></div><div class="code"><pre><code id="code_31" name="py_code">    data_mask = torch.as_tensor([mask[i] for i in action.action_type]).bool()</code></pre></div></div><div class="section" id="section-32"><div class="docs doc-strings"><p>    Filter corresponding action_args according to mask and re-assign it.</p></div><div class="code"><pre><code id="code_32" name="py_code">    filtered_action_args = ttorch.masked_select(action.action_args, data_mask)
    action.action_args = filtered_action_args
    assert action.action_args.shape == (B, )</code></pre></div></div><div class="section" id="section-33"><div class="docs doc-strings"><p>    Select some samples (for example)</p></div><div class="code"><pre><code id="code_33" name="py_code">    selected_action = action[1:3]
    assert selected_action.action_type.shape == (2, )

</code></pre></div></div><div class="section" id="section-33"><div class="docs doc-strings"><p><i>If you have any questions or advices about this documation, you can raise issues in GitHub (https://github.com/opendilab/PPOxFamily) or email us (opendilab@pjlab.org.cn).</i></p></div></div></body><script type="text/javascript">
window.onload = function(){
    var codeElement = document.getElementsByName('py_code');
    var lineCount = 1;
    for (var i = 0; i < codeElement.length; i++) {
        var code = codeElement[i].innerText;
        if (code.length <= 1) {
            continue;
        }

        codeElement[i].innerHTML = "";

        var codeMirror = CodeMirror(
          codeElement[i],
          {
            value: code,
            mode: "python",
            theme: "solarized dark",
            lineNumbers: true,
            firstLineNumber: lineCount,
            readOnly: false,
            lineWrapping: true,
          }
        );
        var noNewLineCode = code.replace(/[\r\n]/g, "");
        lineCount += code.length - noNewLineCode.length + 1;
    }
};
</script></html>