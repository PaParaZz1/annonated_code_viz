<!DOCTYPE html>
<html><head><meta charset="utf-8"></meta><title>Annonated Algorithm Visualization</title><link rel="stylesheet" href="pylit.css?v=1"></link><link rel="stylesheet" href="solarized.css"></link><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css" integrity="sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/" crossorigin="anonymous"></link><script src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js" integrity="sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);" defer="True"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/lib/codemirror.min.css"></link><script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/lib/codemirror.min.js"></script><script src="https://cdn.jsdelivr.net/npm/codemirror@5.61.0/mode/python/python.min.js"></script></head><body><div class="section" id="section-0"><div class="docs doc-strings"><p><p><a href="index.html"><b>HOME<br></b></a></p></p><a href="https://github.com/opendilab/PPOxFamily" target="_blank"><img alt="GitHub" style="max-width:100%;" src="https://img.shields.io/github/stars/opendilab/PPOxFamily?style=social"></img></a>  <a href="https://space.bilibili.com/1112854351?spm_id_from=333.337.0.0" target="_blank"><img alt="bilibili" style="max-width:100%;" src="https://img.shields.io/badge/bilibili-video%20course-blue"></img></a>  <a href="https://twitter.com/OpenDILab" rel="nofollow" target="_blank"><img alt="twitter" style="max-width:100%;" src="https://img.shields.io/twitter/follow/opendilab?style=social"></img></a><br><a href="https://github.com/opendilab/PPOxFamily/tree/main/chapter2_action/discrete_tutorial.py" target="_blank">View code on GitHub</a><br><br>PyTorch tutorial of <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">Proximal Policy Optimization (PPO)</span>  algorithm for discrete action.<br><a href="https://arxiv.org/pdf/1707.06347.pdf">Related Link</a><br><br>PPO is one of the most popular policy gradient methods for deep reinforcement learning. It combines the classic Actor-Critic paradigm and the trust region policy optimization method into a simple yet effect algorithm design. Compared to some traditional RL algorithms like REINFORCE and A2C, PPO can deploy more stable and efficient policy optimization by using clipped surrogate objective mentioned below:<br>$$J(\theta) = \min(\frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{\theta_k}(a_{t}|s_{t})}A^{\theta_k}(s_{t},a_{t}),\text{clip}(\frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{\theta_k}(a_{t}|s_{t})}, 1-\epsilon,1+\epsilon)A^{\theta_k}(s_{t},a_{t}))$$<br>The final objective is a lower bound (i.e., a pessimistic bound) on the unclipped objective, which only ignore the change in probability ratio when it would make the objective improve, and we include it when it makes the objective worse.<br>Detailed notation definition can be found in <a href="https://github.com/opendilab/PPOxFamily/blob/main/chapter1_overview/chapter1_notation.pdf">Related Link</a>.<br><br>Discrete action space, one of the most commonly used action spaces, is often used in video games such as Super Mario Bros, Atari and Procgen. It contains a group of possible discrete action choices and RL agent needs to select one action from them every execution. Discrete action space is often modelled by categorical distribution (classification problem).<br><br>This tutorial is mainly composed of the following three parts, you can learn from these demo codes step by step or using them as code segment in your own program:<br>  - Policy Network Architecture<br>  - Sample Action Function<br>  - Main (Test) Function<br>More visulization results about PPO in discrete action space can be found in <a href="https://github.com/opendilab/PPOxFamily/issues/4">Related Link</a>.</div></div><div class="section" id="section-1"><div class="docs doc-strings"><p>        <b>Overview</b><br>            The definition of discrete action policy network used in PPO, which is mainly composed of two parts: encoder and head.</p></div><div class="code"><pre><code id="code_1" name="py_code">import torch
import torch.nn as nn


class DiscretePolicyNetwork(nn.Module):
    def __init__(self, obs_shape: int, action_shape: int) -> None:</code></pre></div></div><div class="section" id="section-3"><div class="docs doc-strings"><p>        PyTorch necessary requirements for extending <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">nn.Module</span> . Our network should also subclass this class.</p></div><div class="code"><pre><code id="code_3" name="py_code">        super(DiscretePolicyNetwork, self).__init__()</code></pre></div></div><div class="section" id="section-4"><div class="docs doc-strings"><p>        Define encoder module, which maps raw state into embedding vector.<br>        It could be different for various state, such as Convolution Neural Network (CNN) for image state and Multilayer perceptron (MLP) for vector state, respectively.<br>        Here we use one-layer MLP for vector state, i.e.<br>        $$y = max(Wx+b, 0)$$</p></div><div class="code"><pre><code id="code_4" name="py_code">        self.encoder = nn.Sequential(
            nn.Linear(obs_shape, 32),
            nn.ReLU(),
        )</code></pre></div></div><div class="section" id="section-5"><div class="docs doc-strings"><p>        Define discrete action logit output network, just one-layer FC, i.e.<br>        $$y=Wx+b$$</p></div><div class="code"><pre><code id="code_5" name="py_code">        self.head = nn.Linear(32, action_shape)
</code></pre></div></div><div class="section" id="section-6"><div class="docs doc-strings"><p>        <b>Overview</b><br>            The computation graph of discrete action policy network used in PPO.<br>            <span style="color:#00cbf694;font-family:Monaco,IBMPlexMono;">x -> encoder -> head -> logit</span> .</p></div><div class="code"><pre><code id="code_6" name="py_code">    def forward(self, x: torch.Tensor) -> torch.Tensor:</code></pre></div></div><div class="section" id="section-8"><div class="docs doc-strings"><p>        Transform original state into embedding vector, i.e. $$(B, *) -> (B, N)$$</p></div><div class="code"><pre><code id="code_8" name="py_code">        x = self.encoder(x)</code></pre></div></div><div class="section" id="section-9"><div class="docs doc-strings"><p>        Calculate logit for each possible discrete action choices, i.e. $$(B, N) -> (B, A)$$</p></div><div class="code"><pre><code id="code_9" name="py_code">        logit = self.head(x)
        return logit

</code></pre></div></div><div class="section" id="section-10"><div class="docs doc-strings"><p>    <b>Overview</b><br>        The function of sampling discrete action, input shape = (B, action_shape), output shape = (B, ).<br>        In this example, the distributions shapes are:<br>        batch_shape = (B, ), event_shape = (), sample_shape = ().</p></div><div class="code"><pre><code id="code_10" name="py_code">def sample_action(logit: torch.Tensor) -> torch.Tensor:</code></pre></div></div><div class="section" id="section-12"><div class="docs doc-strings"><p>    Transform logit (raw output of policy network, e.g. last fully connected layer) into probability.<br>    $$\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}$$</p></div><div class="code"><pre><code id="code_12" name="py_code">    prob = torch.softmax(logit, dim=-1)</code></pre></div></div><div class="section" id="section-13"><div class="docs doc-strings"><p>    Construct categorical distribution. The probability mass function is: $$f(x=i|\boldsymbol{p})=p_i$$<br>    <a href="https://en.wikipedia.org/wiki/Categorical_distribution">Related Link</a></p></div><div class="code"><pre><code id="code_13" name="py_code">    dist = torch.distributions.Categorical(probs=prob)</code></pre></div></div><div class="section" id="section-14"><div class="docs doc-strings"><p>    Sample one discrete action per sample (state input) and return it.</p></div><div class="code"><pre><code id="code_14" name="py_code">    return dist.sample()

</code></pre></div></div><div class="section" id="section-15"><div class="docs doc-strings"><p>    <b>Overview</b><br>        The function of testing sampling discrete action. Construct a naive policy and sample a group of action.</p></div><div class="code"><pre><code id="code_15" name="py_code">def test_sample_action():</code></pre></div></div><div class="section" id="section-17"><div class="docs doc-strings"><p>    Set batch_size = 4, obs_shape = 10, action_shape = 6.</p></div><div class="code"><pre><code id="code_17" name="py_code">    B, obs_shape, action_shape = 4, 10, 6</code></pre></div></div><div class="section" id="section-18"><div class="docs doc-strings"><p>    Generate state data from uniform distribution in [0, 1].</p></div><div class="code"><pre><code id="code_18" name="py_code">    state = torch.rand(B, obs_shape)</code></pre></div></div><div class="section" id="section-19"><div class="docs doc-strings"><p>    Define policy network with encoder and head.</p></div><div class="code"><pre><code id="code_19" name="py_code">    policy_network = DiscretePolicyNetwork(obs_shape, action_shape)</code></pre></div></div><div class="section" id="section-20"><div class="docs doc-strings"><p>    Policy network forward procedure, input state and output logit.<br>    $$ logit = \pi(a|s)$$</p></div><div class="code"><pre><code id="code_20" name="py_code">    logit = policy_network(state)
    assert logit.shape == (B, action_shape)</code></pre></div></div><div class="section" id="section-21"><div class="docs doc-strings"><p>    Sample action accoding to corresponding logit.</p></div><div class="code"><pre><code id="code_21" name="py_code">    action = sample_action(logit)
    assert action.shape == (B, )

</code></pre></div></div><div class="section" id="section-21"><div class="docs doc-strings"><p><i>If you have any questions or advices about this documation, you can raise issues in GitHub (https://github.com/opendilab/PPOxFamily) or email us (opendilab@pjlab.org.cn).</i></p></div></div></body><script type="text/javascript">
window.onload = function(){
    var codeElement = document.getElementsByName('py_code');
    var lineCount = 1;
    for (var i = 0; i < codeElement.length; i++) {
        var code = codeElement[i].innerText;
        if (code.length <= 1) {
            continue;
        }

        codeElement[i].innerHTML = "";

        var codeMirror = CodeMirror(
          codeElement[i],
          {
            value: code,
            mode: "python",
            theme: "solarized dark",
            lineNumbers: true,
            firstLineNumber: lineCount,
            readOnly: false,
            lineWrapping: true,
          }
        );
        var noNewLineCode = code.replace(/[\r\n]/g, "");
        lineCount += code.length - noNewLineCode.length + 1;
    }
};
</script></html>